{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-04T08:26:30.717308Z","iopub.execute_input":"2022-06-04T08:26:30.718208Z","iopub.status.idle":"2022-06-04T08:26:30.730134Z","shell.execute_reply.started":"2022-06-04T08:26:30.718166Z","shell.execute_reply":"2022-06-04T08:26:30.729363Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"import collections\nimport jieba\nimport re\nimport json\nclass Vocab:\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None, dict_path = None):\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        # The index for the padding word is 0\n        if dict_path is not None:\n            #it can only load full dict.\n            f = open(dict_path,'r')\n            self._token_freqs = [tuple(line.split()) for line in list(f)]\n            self._token_freqs = [(item[0],int(item[-1])) for item in self._token_freqs]\n        else:\n            # Sort according to frequencies\n            counter = count_corpus(tokens)\n            self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n        self.idx_to_token = ['<pad>'] + reserved_tokens\n        self.token_to_idx = {token: idx\n                             for idx, token in enumerate(self.idx_to_token)}\n        for token, freq in self._token_freqs:\n            if freq < min_freq:\n                break\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) - 1\n        print(len(self.token_to_idx))\n    def __len__(self):\n        return len(self.idx_to_token)\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n    @property\n    def unk(self):  # Index for the unknown token\n        return 0\n    @property\n    def token_freqs(self):\n        return self._token_freqs\n\ndef count_corpus(tokens):\n    # Here `tokens` is a 1D list or 2D list\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        # Flatten a list of token lists into a list of tokens\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)\n\ndef clean_str(string):\n    for ch in '!\"#$&*+，、-.:；。 <=>？@[]《》^！_\\|·~‘’':\n        string = string.replace(ch, \"\")\n    return string\ndef read_words(data_file):  #@save\n    with open(data_file) as f:\n        lines = f.readlines()\n        lines.pop(0)\n        #移除标题\n    return [list(jieba.cut(clean_str(re.sub(\"\\t\\d\\n\",\"\",line)))) for line in lines]\n\n# data_file = \"train_SENT.tsv\"\n# tokens = read_words(data_file)\n# vocab = Vocab(reserved_tokens=['<unk>'],dict_path=\"word_freq.txt\")\n# vocab = Vocab(tokens,reserved_tokens=['<unk>'])\n# ASAP_dict = json.dumps(vocab.token_to_idx,sort_keys=False,indent=4,separators=(',',': '))\n# #保存\n# fileObject = open('idx_to_token_bug.txt', 'w')\n# for token in vocab.idx_to_token:\n#     fileObject.write(str(token))\n#     fileObject.write('\\n')\n# fileObject.close()\n# fileObject = open('token_to_idx.json','w')\n# fileObject.write(ASAP_dict)\n# fileObject.close()\n# fileObject = open('word_freq.txt','w')\n# for freq in vocab._token_freqs:\n#     fileObject.write(f\"{freq[0]} {freq[1]}\")\n#     fileObject.write('\\n')\n# fileObject.close()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T08:26:30.787097Z","iopub.execute_input":"2022-06-04T08:26:30.787579Z","iopub.status.idle":"2022-06-04T08:26:30.804947Z","shell.execute_reply.started":"2022-06-04T08:26:30.787546Z","shell.execute_reply":"2022-06-04T08:26:30.803959Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"# Train Utils","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport jieba\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch import nn\nfrom torchtext.vocab import Vectors\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass CrossEntropyLoss_LS(nn.Module):\n    ''' Cross Entropy Loss with label smoothing '''\n    def __init__(self, label_smooth=None, class_num=5):\n        super().__init__()\n        self.label_smooth = label_smooth\n        self.class_num = class_num\n        self.eps = 1e-12\n    def forward(self, pred, target):\n        '''\n        Args:\n            pred: prediction of model output [batch_size, num_classe]\n            target: ground truth of sampler [num_classes]\n        '''\n        if self.label_smooth is not None:\n            # cross entropy loss with label smoothing\n            logprobs = F.log_softmax(pred, dim=1)  # softmax + log\n            target = F.one_hot(target, self.class_num)  # 转换成one-hot\n            target = torch.clamp(target.float(), min=self.label_smooth / (self.class_num-1),\n                                 max=1.0 - self.label_smooth)\n            loss = -1 * torch.sum(target * logprobs, 1)\n        else:\n            # standard cross entropy loss\n            loss = -1. * pred.gather(1, target.unsqueeze(-1)) + torch.log(torch.exp(pred + self.eps).sum(dim=1))\n        return loss.mean()\nclass collector():\n    def __init__(self, indexes):\n        self.data = {}\n        for index in indexes:\n            self.data[index] = []\n    def genDataFrame(self):\n        return pd.DataFrame(self.data)\n    def genCSV(self, name):\n        pd.DataFrame(self.data).to_csv(name,encoding='utf-8',index=False)\n    def append(self, item, index):\n        self.data[index].append(item)\ndef evaluateTSV(model, state_dict_path, dataloader, loss_func, device, outputPath=None,join=False):\n    model.eval()\n    if outputPath is not None:\n        f = open(outputPath, \"w\")\n    if state_dict_path is not None:\n        model.load_state_dict(torch.load(state_dict_path,map_location=device))\n    avg_loss = 0\n    correct = 0\n    total = 0\n    if outputPath is not None:\n        f.write(\"index\\tprediction\\n\")\n    i = 0\n    for X,keyword,y in tqdm(dataloader):\n        X = X.to(device)\n        y = y.to(device)\n        keyword = keyword.to(device)\n        output = model(X,keyword)\n        test_loss = loss_func(output,y)\n        avg_loss += test_loss.item()\n        output = output.argmax(1)\n        if join:\n            output = output // 2.5\n        result = output == y\n        correct += result.sum().item()\n        total += output.shape[0]\n        if outputPath is not None:\n            if output.shape == 1:\n                f.write(f\"{i}\\t{output.item()-1}\\n\")\n                i += 1\n            else:\n                for item in output:\n                    f.write(f\"{i}\\t{item.item()-1}\\n\")\n                    i += 1\n\n\n    acc = correct/total*100\n    avg_loss /= len(dataloader)\n    print(f\"Avg Test Loss:{avg_loss:>5.3f}\")\n    print(f\"Accuracy: {acc:<6.2f}%\")\n    if outputPath is not None:\n        f.close()\n    model.train()\n    return avg_loss,acc\ndef evaluateSENT(model, state_dict_path, dataloader, loss_func, device, outputPath=None, FullLabel=False, Reg=False):\n    model.eval()\n    if outputPath is not None:\n        f = open(outputPath, \"w\")\n    if state_dict_path is not None:\n        model.load_state_dict(torch.load(state_dict_path,map_location=device))\n    avg_loss = 0\n    correct = 0\n    total = 0\n    if outputPath is not None:\n        f.write(\"index\\tprediction\\tcorrect\\n\")\n    i = 0\n    for X,y in tqdm(dataloader):\n        X = X.to(device)\n        y = y.to(device)\n        output = model(X)\n        if Reg:\n            if FullLabel:\n                test_loss = loss_func(output.squeeze(1),y[:,0])\n            else:\n                test_loss = loss_func(output,y)\n        else:\n            output = torch.round(output)\n            if FullLabel:\n                test_loss = loss_func(output.squeeze(1), y[:, 0])\n                result = output == y[:, 0]\n            else:\n                test_loss = loss_func(output, y)\n                result = output == y\n            correct += result.sum().item()\n            total += output.shape[0]\n        avg_loss += test_loss.item()\n        if outputPath is not None:\n            if output.shape == 1:\n                f.write(f\"{i}\\t{output.item()}\\n\")\n                # f.write(f\"{i}\\t{output.item()}\\t{y.squeeze(1).item()}\\n\")\n                i += 1\n            else:\n                for item in output:\n                    f.write(f\"{i}\\t{item.item()}\\n\")\n                    # f.write(f\"{i}\\t{output.item()}\\t{y.squeeze(1).item()}\\n\")\n                    i += 1\n    avg_loss /= len(dataloader)\n    print(f\"Avg Test Loss:{avg_loss:>5.3f}\")\n    if not Reg:\n        acc = correct / total * 100\n        print(f\"Accuracy: {acc:<6.2f}%\")\n    if outputPath is not None:\n        f.close()\n    model.train()\n    if Reg:\n        return avg_loss\n    else:\n        return avg_loss,acc\ndef evaluateSENT_CLS(model, state_dict_path, dataloader, loss_func, device, outputPath=None, FullLabel=False):\n    model.eval()\n    if outputPath is not None:\n        f = open(outputPath, \"w\")\n    if state_dict_path is not None:\n        model.load_state_dict(torch.load(state_dict_path,map_location=device))\n    avg_loss = 0\n    correct = 0\n    total = 0\n    if outputPath is not None:\n        f.write(\"index\\tprediction\\tlabel\\n\")\n    i = 0\n    for X,y in tqdm(dataloader):\n        X = X.to(device)\n        y = y = (y-1).to(device).to(torch.long)\n        output = model(X)\n        predict = output.argmax(1)\n        if FullLabel:\n            test_loss = loss_func(output, y[:, 0])\n            result = predict == y[:, 0]\n        else:\n            test_loss = loss_func(output, y)\n            result = predict == y\n        correct += result.sum().item()\n        total += output.shape[0]\n        avg_loss += test_loss.item()\n        if outputPath is not None:\n            if predict.shape == 1:\n                f.write(f\"{i}\\t{predict.item()}\\t{y[0][0].item()}\\n\")\n                i += 1\n            else:\n                for item in predict:\n                    f.write(f\"{i}\\t{item.item()}\\t{y[0][0].item()}\\n\")\n                    i += 1\n    avg_loss /= len(dataloader)\n    print(f\"Avg Test Loss:{avg_loss:>5.3f}\")\n    acc = correct / total * 100\n    print(f\"Accuracy: {acc:<6.2f}%\")\n    if outputPath is not None:\n        f.close()\n    model.train()\n    return avg_loss,acc\nclass ASAP_ASPECT_DICT(torch.utils.data.Dataset):\n    def __init__(self, data_file, dict_obj, aspect_dict,mode=\"train\"):\n        self.raw_data = pd.read_csv(data_file,sep='\\t')\n        self.vocab = dict_obj\n        self.aspect_dict = aspect_dict\n        self.label_dict = {\"text_a\":1,\"cate\":2,\"label\":3}\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.raw_data)\n    def padVector(self):\n        return 0\n    def clean_str(self, string):\n        for ch in '!\"#$&*+，、-.:；<=>？@[]《》^_|·~‘’':\n            string = string.replace(ch,\"\")\n        return string\n    def __getitem__(self, idx):\n        line = self.raw_data.iloc[idx]\n        indexes = [self.label_dict[\"cate\"],self.label_dict[\"label\"],self.label_dict[\"text_a\"]]\n        if self.mode == \"train\":\n            for i in range(len(indexes)):\n                indexes[i] -= 1\n        keyword = self.aspect_dict[line[indexes[0]]]\n        label = torch.tensor(line[indexes[1]])\n        seg_list = list(jieba.cut(self.clean_str(line[indexes[2]])))\n        sentence = []\n        for word in seg_list:\n            if word in self.vocab.token_to_idx:\n                vector_index = self.vocab[word]\n                sentence.append(vector_index)\n            else:\n                # For Unknown words.\n                sentence.append(1)\n        return torch.as_tensor(sentence), label+1, keyword\nclass ASAP_WIKI(torch.utils.data.Dataset):\n    def __init__(self, data_file, vec_file, max_seq_length,aspect_dict,mode=\"train\"):\n        self.raw_data = pd.read_csv(data_file,sep='\\t')\n        self.wordvec = Vectors(name=vec_file)\n        self.max_seq_length = max_seq_length\n        self.aspect_dict = aspect_dict\n        self.label_dict = {\"text_a\":1,\"cate\":2,\"label\":3}\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.raw_data)\n    def padVector(self):\n        return self.wordvec.stoi['。']\n    def clean_str(self, string):\n        for ch in '!\"#$&*+，、-.:；<=>？@[]《》^_|·~‘’':\n            string = string.replace(ch,\"\")\n        return string\n    def __getitem__(self, idx):\n        line = self.raw_data.iloc[idx]\n        indexes = [self.label_dict[\"cate\"],self.label_dict[\"label\"],self.label_dict[\"text_a\"]]\n        if self.mode == \"train\":\n            for i in range(len(indexes)):\n                indexes[i] -= 1\n        keyword = self.aspect_dict[line[indexes[0]]]\n        label = torch.tensor(line[indexes[1]])\n        seg_list = list(jieba.cut(self.clean_str(line[indexes[2]])))\n        sentence = []\n        for word in seg_list[:min(len(seg_list),self.max_seq_length)]:\n            if word in self.wordvec.stoi:\n                vector_index = self.wordvec.stoi[word]\n                sentence.append(vector_index)\n            else:\n                sentence.append(0)\n        return torch.tensor(sentence), label+1, keyword\nclass ASAP_Full(torch.utils.data.Dataset):\n    def __init__(self, data_file, dictObj):\n        self.raw_data = pd.read_csv(data_file)\n        self.lookupTable = dictObj;\n\n    def __len__(self):\n        return len(self.raw_data)\n    def clean_str(self, string):\n        for ch in '!\"#$&*+，、-.:；<=>？@[]《》^_|·~‘’':\n            string = string.replace(ch,\"\")\n        return string\n    def __getitem__(self, idx):\n        line = self.raw_data.iloc[idx]\n        label = torch.tensor(line[2:])\n        seg_list = list(jieba.cut(self.clean_str(line[1])))\n        sentence = []\n        for word in seg_list:\n            if word in self.lookupTable.token_to_idx:\n                vector_index = self.lookupTable[word]\n                sentence.append(vector_index)\n            else:\n                sentence.append(1)\n        mask = label > -2\n        return torch.tensor(sentence), label * mask\nclass ASAP_SENT_DICT(torch.utils.data.Dataset):\n    def __init__(self, data_file, dict_obj, mode=\"train\"):\n        self.raw_data = pd.read_csv(data_file, sep='\\t')\n        self.label_dict = {\"text_a\": 1, \"star\": 2}\n        self.vocab = dict_obj\n        self.mode = mode\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def padVector(self):\n        return 0\n\n    def clean_str(self, string):\n        for ch in '!\"#$&*+，、-.:；<=>？@[]《》^_|·~‘’':\n            string = string.replace(ch, \"\")\n        return string\n\n    def __getitem__(self, idx):\n        line = self.raw_data.iloc[idx]\n        indexes = [self.label_dict[\"text_a\"], self.label_dict[\"star\"]]\n        if self.mode == \"train\":\n            for i in range(len(indexes)):\n                indexes[i] -= 1\n        if self.mode == \"test\":\n            label = torch.tensor(0)\n        else:\n            label = torch.tensor(line[indexes[1]])\n        seg_list = list(jieba.cut(self.clean_str(line[indexes[0]])))\n        sentence = []\n        for word in seg_list:\n            if word in self.vocab.token_to_idx:\n                vector_index = self.vocab[word]\n                sentence.append(vector_index)\n            else:\n                #For Unknown words.\n                sentence.append(1)\n        # if len(seg_list)<self.max_seq_length:\n        #     sentence = sentence + [self.padVector()]*(self.max_seq_length-len(seg_list))\n        return torch.as_tensor(sentence), label\ndef collcate_pad(data):\n    reviews = [item[0] for item in data]\n    reviews = torch.as_tensor(pad_sequence(reviews,batch_first=True))\n    labels = torch.as_tensor([item[1] for item in data])\n    cates = torch.as_tensor([item[2] for item in data])\n    return reviews,cates,labels\ndef collcate_pad_SENT(data):\n    reviews = [item[0] for item in data]\n    reviews = torch.as_tensor(pad_sequence(reviews, batch_first=True))\n    labels = torch.tensor([item[1] for item in data])\n    return reviews, labels\ndef collcate_pad_Full(data):\n    reviews = [item[0] for item in data]\n    reviews = torch.as_tensor(pad_sequence(reviews, batch_first=True))\n    labels = torch.cat([item[1] for item in data]).reshape(len(data),-1)\n    return reviews,labels\ndef get_aspect_vector_DICT(vocabObj):\n    aspect_map = {'Location#Transportation':\"交通\", 'Location#Downtown':\"商圈\",\n                   'Location#Easy_to_find':\"定位\", 'Service#Queue':\"排队\", 'Service#Hospitality':\"服务\",\n                   'Service#Parking':\"停车\", 'Service#Timely':\"准时\", 'Price#Level':\"价格\",\n                   'Price#Cost_effective':\"性价比\", 'Price#Discount':\"折扣\", 'Ambience#Decoration':\"装修\",\n                   'Ambience#Noise':\"噪音\", 'Ambience#Space':\"空间\", 'Ambience#Sanitary':\"卫生\", 'Food#Portion':\"分量\",\n                   'Food#Taste':\"口味\", 'Food#Appearance':\"外观\", 'Food#Recommend':\"推荐\"}\n    aspect_indexes = []\n    wordvec_dict = {}\n    for key in aspect_map:\n        indexKey = torch.tensor(vocabObj.token_to_idx[aspect_map[key]])\n        aspect_indexes.append(indexKey)\n        wordvec_dict[key] = indexKey\n    return aspect_indexes,wordvec_dict","metadata":{"execution":{"iopub.status.busy":"2022-06-04T08:26:31.038715Z","iopub.execute_input":"2022-06-04T08:26:31.039210Z","iopub.status.idle":"2022-06-04T08:26:31.109066Z","shell.execute_reply.started":"2022-06-04T08:26:31.039176Z","shell.execute_reply":"2022-06-04T08:26:31.108205Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"from torchtext.vocab import Vectors\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nclass GCAE_ACSA(nn.Module):\n    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels, aspect_indexes, num_classes=3):\n        super(GCAE_ACSA, self).__init__()\n        self.embedding1 = nn.Embedding(vocab_size, embed_size)\n        self.embedding2 = nn.Embedding(vocab_size, embed_size)\n        self.aspectFC = nn.Linear(2 * embed_size,num_channels[0])\n        self.sentConv = nn.ModuleList()\n        self.aspectConv = nn.ModuleList()\n        for c, k in zip(num_channels, kernel_sizes):\n            self.sentConv.append(nn.Conv1d(2 * embed_size, c, k))\n            self.aspectConv.append(nn.Conv1d(2 * embed_size, c, k))\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.classifyFC = nn.Linear(sum(num_channels),num_classes)\n        self.aspectIndexes = aspect_indexes\n    def forward(self, x, key=None):\n        embeddings = torch.cat((self.embedding1(x), self.embedding2(x)), dim=2).transpose(1,2)\n        sent_feat = torch.cat([torch.squeeze(torch.tanh(self.pool(conv(embeddings))), dim=-1)\n            for conv in self.sentConv], dim=1)\n        if key is not None:\n            aspect_embed = torch.cat((self.embedding1(key), self.embedding2(key)), dim=1)\n            aspect_feat = torch.cat([self.relu(torch.squeeze(self.pool(conv(embeddings)), dim=-1)+self.aspectFC(aspect_embed)) for conv in self.aspectConv], dim=1)\n            out = sent_feat * aspect_feat\n        else:\n            out = sent_feat\n        out = self.classifyFC(self.dropout(out))\n        return out\nclass TextCNN(nn.Module):\n    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n                 **kwargs):\n        super(TextCNN, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        # The embedding layer not to be trained\n        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n        self.dropout = nn.Dropout(0.5)\n        self.decoder = nn.Linear(sum(num_channels), 3)\n        # The max-over-time pooling layer has no parameters, so this instance\n        # can be shared\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.relu = nn.ReLU()\n        # Create multiple one-dimensional convolutional layers\n        self.convs = nn.ModuleList()\n        for c, k in zip(num_channels, kernel_sizes):\n            self.convs.append(nn.Conv1d(2 * embed_size, c, k))\n\n    def forward(self, inputs, dummy=None):\n        # Concatenate two embedding layer outputs with shape (batch size, no.\n        # of tokens, token vector dimension) along vectors\n        embeddings = torch.cat((\n            self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n        # Per the input format of one-dimensional convolutional layers,\n        # rearrange the tensor so that the second dimension stores channels\n        embeddings = embeddings.permute(0, 2, 1)\n        # For each one-dimensional convolutional layer, after max-over-time\n        # pooling, a tensor of shape (batch size, no. of channels, 1) is\n        # obtained. Remove the last dimension and concatenate along channels\n        encoding = torch.cat([\n            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n            for conv in self.convs], dim=1)\n        outputs = self.decoder(self.dropout(encoding))\n        return outputs\ndef init_weights(item):\n    if isinstance(item,nn.Conv1d) or isinstance(item,nn.Linear):\n        torch.nn.init.xavier_uniform_(item.weight)\n        item.bias.data.fill_(0.01)\n    if type(item) == nn.LSTM:\n        for param in item._flat_weights_names:\n            if \"weight\" in param:\n                nn.init.xavier_uniform_(item._parameters[param])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T08:26:31.111135Z","iopub.execute_input":"2022-06-04T08:26:31.111504Z","iopub.status.idle":"2022-06-04T08:26:31.133265Z","shell.execute_reply.started":"2022-06-04T08:26:31.111468Z","shell.execute_reply":"2022-06-04T08:26:31.132303Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# Train Function","metadata":{}},{"cell_type":"code","source":"def train_ASPECT(model, dataloader, test_dataloader, loss_func, optimizer, epochs, device, save_name, logger,join = False):\n    t0 = time.time()\n    plot_loss = collector([\"epoch\",\"train_loss\",\"val_loss\"])\n    plot_acc = collector([\"epoch\",\"acc\"])\n    model = model.to(device)\n    best_acc = -1\n    best_epoch = -1\n    for epoch in range(epochs):\n        print('='*10 + f\"epoch {epoch+1}\" + '='*10)\n        avg_loss = 0\n        step = 0\n        total = len(dataloader)\n        for X,keyword,y in tqdm(dataloader):\n            X = X.to(device)\n            y = y.to(torch.long).to(device)\n            keyword = keyword.to(device)\n            output = model(X,keyword)\n            if join:\n                for i in range(y.shape[0]):\n                    if y[i]==1:\n                        y[i] = torch.randint(1,3,(1,))\n                    elif y[i]==2:\n                        y[i] = torch.randint(3,5,(1,))\n            train_loss = loss_func(output,y)\n            avg_loss += train_loss.item()\n            optimizer.zero_grad()\n            train_loss.backward()\n            optimizer.step()\n            step += 1\n            if step%50==0:\n                print(f\"Step {step}/{total} Batch Loss: {train_loss.item():>5.3f}\")\n                logger.info(f\"Step {step}/{total} Batch Loss: {train_loss.item():>5.3f}\")\n        avg_loss /= len(dataloader)\n        print(f\"Avg Train Loss of Epoch {epoch+1}: {avg_loss:>5.3f}\")\n        logger.info(f\"Avg Train Loss of Epoch {epoch+1}:{avg_loss:>5.3f}\")\n        result = evaluateTSV(model, None ,test_dataloader, loss_func, device, join=False)\n        plot_loss.append(epoch+1,\"epoch\")\n        plot_loss.append(avg_loss,\"train_loss\")\n        plot_loss.append(result[0],\"val_loss\")\n        plot_acc.append(epoch+1,\"epoch\")\n        plot_acc.append(result[1],\"acc\")\n        logger.info(f\"Acc: {result[1]:<6.2f}%\")\n        if result[1]>best_acc:\n            best_acc = result[1]\n            best_epoch = epoch + 1\n            bestCallOut = f\"Best acc {best_acc:<6.2f}% is at Epoch {best_epoch}\"\n            print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + bestCallOut)\n            logger.info(bestCallOut)\n            torch.save(model.state_dict(), save_name + '.pt')\n            plot_acc.genCSV(save_name + \" acc_evo.csv\")\n        plot_loss.genCSV(save_name + \" loss_evo.csv\")\n    print(\"Training Completed. With \" + bestCallOut + f\" Total Time:{time.time() - t0:>5.2f}s\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-04T08:26:31.134468Z","iopub.execute_input":"2022-06-04T08:26:31.135018Z","iopub.status.idle":"2022-06-04T08:26:31.151851Z","shell.execute_reply.started":"2022-06-04T08:26:31.134967Z","shell.execute_reply":"2022-06-04T08:26:31.151115Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"import torch\nimport logging\nimport time\nimport datetime\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n#Config Logger\ndatasetName = \"trian.tsv\"\nsave_name = datasetName + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\nLog_Format = \"%(levelname)s %(asctime)s - %(message)s\"\nfor handler in logging.root.handlers[:]:\n    logging.root.removeHandler(handler)\nlogging.basicConfig(\n    filename=save_name + '.log',\n    filemode=\"w\",\n    format=Log_Format,\n    level=logging.INFO)\nlogger = logging.getLogger()\n\n#Environment Configuration\nlogger.info(torch.__version__)\nimport numpy as np\nlogger.info(np.__version__)\nimport sys\nlogger.info(sys.version)\n\n#Hyperparameters\nwordvec_length = 50\nepochs = 30\nrandom_seed = \"random\"\n# random_seed = 500\n# torch.manual_seed(random_seed)\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n# wordvec_path = \"/Users/Charlie/Desktop/sgns.wiki.bigram\"\ngpu_boost = 1\nbase_lr = 4e-4\nlearning_rate = base_lr*gpu_boost if device != 'cpu' else base_lr\nbase_batch = 64\nbatch_size = base_batch*gpu_boost if device != 'cpu' else base_batch\npool_length = 1\nnum_aspects = 18\nmin_freq = 5\n# wiki_vec_dict = 352217\ninput_path = \"/kaggle/input/meituan-asap/\"\nloss_func = CrossEntropyLoss_LS(label_smooth=0.10,class_num=3)\nkernel_sizes, num_channels = [3, 4, 5], [64, 64, 64]\n#Load Data\nvocab = Vocab(reserved_tokens=['<unk>'],min_freq=min_freq,dict_path=\"/kaggle/input/meituan-asap-sent/word_freq.txt\")\naspect_indexes,aspect_dict = get_aspect_vector_DICT(vocab)\naspect_indexes = torch.tensor(aspect_indexes).to(device)\ntraining_data = ASAP_ASPECT_DICT(input_path + datasetName,vocab,aspect_dict)\ntest_data = ASAP_ASPECT_DICT(input_path + \"dev.tsv\",vocab,aspect_dict,\"val\")\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size, \\\ncollate_fn=collcate_pad, shuffle=True)\ntest_dataloader = DataLoader(test_data, batch_size=1, collate_fn=collcate_pad)\n#Prepare Model\nNN = TextCNN(len(vocab),wordvec_length,kernel_sizes,num_channels)\nNN.apply(init_weights)\noptimizer = torch.optim.Adam(NN.parameters(),lr=learning_rate)\n#Start Training\nprint('=' * 7 + \"Training Begin\" + '=' * 7)\nprint(\n    f\"total epoch: {epochs}\\ndevice: {device}\\nbatch size: {batch_size}\\nloss func:{loss_func}\\nlr: {learning_rate}\\nrandom_seed: {random_seed}\\ndict size: {len(vocab)}\\nwordvec length: {wordvec_length}\\nfilter num: {num_channels}\\nfilter size: {kernel_sizes}\\npool length: {pool_length}\")\nlogger.info(\n    f\"total epoch: {epochs}\\ndevice: {device}\\nbatch size: {batch_size}\\nloss func:{loss_func}\\nlr: {learning_rate}\\nrandom_seed: {random_seed}\\ndict size: {len(vocab)}\\nwordvec length: {wordvec_length}\\nfilter num: {num_channels}\\nfilter size: {kernel_sizes}\\npool length: {pool_length}\")\nlogger.info(NN)\ntrain_ASPECT(NN,train_dataloader,test_dataloader,loss_func,optimizer,epochs,device,save_name,logger,join=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T08:26:31.155453Z","iopub.execute_input":"2022-06-04T08:26:31.155958Z"},"trusted":true},"execution_count":null,"outputs":[]}]}